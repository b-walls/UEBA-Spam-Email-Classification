{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da42a46a-0e2c-498a-86d2-5511089d620f",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9de6792-5fae-4eff-878f-7dd92f7fd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63131368-8ba9-4576-a009-01a26d4f4091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                            Message\n",
       "0            0  Go until jurong point, crazy.. Available only ...\n",
       "1            0                      Ok lar... Joking wif u oni...\n",
       "2            1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3            0  U dun say so early hor... U c already then say...\n",
       "4            0  Nah I don't think he goes to usf, he lives aro...\n",
       "...        ...                                                ...\n",
       "5567         1  This is the 2nd time we have tried 2 contact u...\n",
       "5568         0               Will ü b going to esplanade fr home?\n",
       "5569         0  Pity, * was in mood for that. So...any other s...\n",
       "5570         0  The guy did some bitching but I acted like i'd...\n",
       "5571         0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data\\spam.csv')\n",
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612e91f-22a9-4ab0-87d8-d13566b17105",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970becd-9c2d-4a75-a7f6-128184d24846",
   "metadata": {},
   "source": [
    "### Frequency of URLS and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7506a56c-94db-4d4e-88c3-200e006d561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def count_url(text):\n",
    "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')\n",
    "    return len(url_pattern.findall(text))\n",
    "\n",
    "df['freq_urls'] = df['Message'].apply(count_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a7145-bf07-4b40-aed5-8c3cf2381462",
   "metadata": {},
   "source": [
    "### Frequency of 'Urgent' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55810802-4d02-46ec-b98f-02a43533924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urgency_words = [\n",
    "    \"immediate\", \"urgent\", \"critical\", \"important\", \"now\", \"ASAP\", \"as soon as possible\",\n",
    "    \"emergency\", \"priority\", \"alert\", \"rush\", \"prompt\", \"hasten\", \"swift\", \"instantly\",\n",
    "    \"right away\", \"without delay\", \"high priority\", \"imminent\", \"pressing\", \"time-sensitive\",\n",
    "    \"expedite\", \"top priority\", \"crucial\", \"vital\", \"necessary\", \"quick\", \"speedy\", \"at once\",\n",
    "    \"rapid\", \"flash\", \"instantaneous\", \"accelerated\", \"breakneck\", \"hurry\", \"immediately\",\n",
    "    \"fast-track\", \"at the earliest\", \"act now\", \"don't delay\", \"on the double\", \"without hesitation\",\n",
    "    \"fast\", \"soon\", \"now or never\", \"urgent action\", \"right now\", \"straightaway\", \"double-time\",\n",
    "    \"speed\", \"express\", \"high-priority\", \"pressing need\", \"at your earliest convenience\", \"this instant\",\n",
    "    \"forthwith\", \"like a shot\", \"snap to it\", \"on the spot\", \"no time to lose\", \"no delay\",\n",
    "    \"in a hurry\", \"right this minute\", \"get going\", \"with haste\"\n",
    "]\n",
    "\n",
    "def count_urgency_words(text, urgency_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    count = sum(1 for word in words if word in urgency_words)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d08b5d-26a5-4398-b259-f72ef5e6060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq_urgent_words'] = df['Message'].apply(count_urgency_words, urgency_words=urgency_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ed789-d73a-45b5-843c-47fd3d336f7b",
   "metadata": {},
   "source": [
    "### Capital run length total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d033efb-402e-4f5e-a373-9497b3563a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_run_length(text, min_length_count=2):\n",
    "    capital_runs = re.findall(r'[A-Z]+', text)\n",
    "    run_lengths = [len(run) for run in capital_runs if len(run) >= min_length_count]\n",
    "    return sum(run_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df2e527a-fd4f-425b-8116-b7f193934ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['capital_run_length_total'] = df['Message'].apply(count_capital_run_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4594ecb-7d64-4e82-86d6-455e9825d418",
   "metadata": {},
   "source": [
    "### important special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5869dec-5fc2-40c1-966d-da1da712c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_special_chars(text, char):\n",
    "    return text.count(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f42aa14-25f8-46fc-84fe-b44c590a8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq_exclamation'] = df['Message'].apply(count_special_chars, char='!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdfd71-696e-40de-b5b2-087481b8f921",
   "metadata": {},
   "source": [
    "## Clean text using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "270917bf-7a37-4300-a6c4-e31bd21aa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# stop words\n",
    "# nltk.download('punkt') <------ need these lines to \n",
    "# nltk.download('stopwords') <-- load stopwords\n",
    "stop_words = stopwords.words()\n",
    "stop_words.append('u')\n",
    "stop_words.append('ur')\n",
    "\n",
    "# lemmatizer initialization\n",
    "# nltk.download('averaged_perceptron_tagger') <---- need these lines to downnload\n",
    "# nltk.download('wordnet') <----------------------- wordnet used for lemmitization\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39eb24-7fb3-4edf-9396-a04d556c9239",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a3c5b8d-87ee-4b75-a84c-e817bbd36ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_email(text, lemmatizer):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "935b61c1-b2dc-4142-8d51-ece32a0f9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message'] = df['Message'].apply(lemmatize_email, lemmatizer=lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849b81a-fd26-4202-bddb-6c2fd8b0d318",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "947c9f77-e822-45a0-a843-fcc68e038434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text, stop_words):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    new_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "838c5277-b6d4-48aa-97b6-c3fc3a903620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message'] = df['Message'].apply(remove_stop_words, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6b2f5-4ff3-4383-9dce-8b7669f010e9",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "768714de-814e-48d0-897f-f441f0242c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# vectorize the emails\n",
    "X = vectorizer.fit_transform(df['Message'])\n",
    "\n",
    "# Create new data frame\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# add engineered features\n",
    "df_bow['freq_urls'] = df['freq_urls']\n",
    "df_bow = df_bow.drop(columns='www') # turns out this column basically has the same counts as url frequency\n",
    "df_bow['freq_urgent_words'] = df['freq_urgent_words']\n",
    "df_bow['freq_exclamation'] = df['freq_exclamation']\n",
    "df_bow['capital_run_length_total'] = df['capital_run_length_total']\n",
    "\n",
    "# add class\n",
    "df_bow['Category'] = df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c4929775-d202-4ab8-9175-85b544ea6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find totals in order to slim features\n",
    "df_bow.loc['Total'] = df_bow.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5d137eff-bf55-4650-8dec-58960fc272b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features with less than n appearances\n",
    "min_num_appearances = 5 # n\n",
    "cols_to_drop = df_bow.columns[df_bow.loc['Total'] < min_num_appearances] \n",
    "df_bow = df_bow.drop(columns=cols_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6c2475ed-35ce-41d7-b749-b7b191cfae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove entries with no appearances of the word freq features while keeping the engineered features intact\n",
    "non_word_freq_columns = {'Category', 'freq_urls', 'freq_urgent_words', 'capital_run_length_total',\n",
    "                         'freq_exclamation'}\n",
    "df_bow = df_bow.loc[~(df_bow[df_bow.drop(columns=non_word_freq_columns).columns] == 0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "44af28be-4e46-4ca0-8cd9-ed85562d2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove total row\n",
    "df_bow = df_bow.drop(index='Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ee43a1e6-1023-487f-814d-6b69c1d971fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_bow = df_bow.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37438f8-d072-4210-963d-1c7408f48281",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f174e504-1bb4-4c7a-966a-27085a2c96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5a5459f9-66f2-4c1d-ace6-acd95cf2e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_bow.drop(columns='Category')\n",
    "y = df_bow['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca849fe-3b45-4899-b8e1-66be5bf65f43",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8fe62-4a39-4b0e-9165-01b5e26dca72",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "35116355-2c9a-4144-a34c-4d2e4a957ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision: 1.0\n",
      "Testing precision: 1.0\n",
      "\n",
      "Classification Report Training:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3587\n",
      "           1       1.00      0.99      1.00       581\n",
      "\n",
      "    accuracy                           1.00      4168\n",
      "   macro avg       1.00      1.00      1.00      4168\n",
      "weighted avg       1.00      1.00      1.00      4168\n",
      "\n",
      "Classification Report Testing:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       880\n",
      "           1       1.00      0.87      0.93       163\n",
      "\n",
      "    accuracy                           0.98      1043\n",
      "   macro avg       0.99      0.93      0.96      1043\n",
      "weighted avg       0.98      0.98      0.98      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2140)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = rfc.predict(X_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(f'Training precision: {precision_score(y_train, y_pred_train)}')\n",
    "print(f'Testing precision: {precision_score(y_test, y_pred)}\\n')\n",
    "print(f'Classification Report Training:\\n {classification_report(y_train, y_pred_train)}')\n",
    "print(f'Classification Report Testing:\\n {classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a2c20464-78e4-467a-8dfe-01025eb07d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.006525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.017372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.007737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.006716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award</th>\n",
       "      <td>0.010894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td>0.036117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cash</th>\n",
       "      <td>0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat</th>\n",
       "      <td>0.005596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.021507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>0.006262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>0.010050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>draw</th>\n",
       "      <td>0.005015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>0.027225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed</th>\n",
       "      <td>0.005566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>0.006269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message</th>\n",
       "      <td>0.005429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.005872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile</th>\n",
       "      <td>0.025706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nokia</th>\n",
       "      <td>0.005213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.013223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate</th>\n",
       "      <td>0.005097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reply</th>\n",
       "      <td>0.011605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ringtone</th>\n",
       "      <td>0.006458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>0.017111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>0.013549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.019258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.007068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt</th>\n",
       "      <td>0.043667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>0.022741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>win</th>\n",
       "      <td>0.006245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_urls</th>\n",
       "      <td>0.029243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_urgent_words</th>\n",
       "      <td>0.015147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_exclamation</th>\n",
       "      <td>0.029345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <td>0.091986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0\n",
       "100                       0.006525\n",
       "1000                      0.007539\n",
       "150p                      0.017372\n",
       "16                        0.007737\n",
       "18                        0.008714\n",
       "50                        0.006716\n",
       "500                       0.010101\n",
       "award                     0.010894\n",
       "call                      0.036117\n",
       "cash                      0.010250\n",
       "chat                      0.005596\n",
       "claim                     0.021507\n",
       "com                       0.006262\n",
       "contact                   0.010050\n",
       "draw                      0.005015\n",
       "free                      0.027225\n",
       "guaranteed                0.005566\n",
       "http                      0.006269\n",
       "message                   0.005429\n",
       "min                       0.005872\n",
       "mobile                    0.025706\n",
       "nokia                     0.005213\n",
       "prize                     0.013223\n",
       "rate                      0.005097\n",
       "reply                     0.011605\n",
       "ringtone                  0.006458\n",
       "service                   0.017111\n",
       "stop                      0.013549\n",
       "text                      0.019258\n",
       "tone                      0.007068\n",
       "txt                       0.043667\n",
       "uk                        0.022741\n",
       "win                       0.006245\n",
       "freq_urls                 0.029243\n",
       "freq_urgent_words         0.015147\n",
       "freq_exclamation          0.029345\n",
       "capital_run_length_total  0.091986"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame(rfc.feature_importances_, index=X.columns)\n",
    "\n",
    "# Displays with values of 0.0 importance dropped\n",
    "feature_importance[(feature_importance >= 0.005).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc6c9b-3883-4ac2-a84e-cbee8abc3997",
   "metadata": {},
   "source": [
    "### KFold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2b544f08-43f7-4e8d-993f-73001b885696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train precision: 0.9992808341599144\n",
      "Test precision: 0.9840050180335582\n",
      "Fold 2\n",
      "Train precision: 0.9997602014388451\n",
      "Test precision: 0.9849148931682521\n",
      "Fold 3\n",
      "Train precision: 0.9995205369556805\n",
      "Test precision: 0.9793943970716257\n",
      "Fold 4\n",
      "Train precision: 0.9992810064943446\n",
      "Test precision: 0.9739870933608937\n",
      "Fold 5\n",
      "Train precision: 0.9992810064943446\n",
      "Test precision: 0.9837083064467287\n",
      "\n",
      "AVERAGE RESULTS:\n",
      "Training Precision: 0.9994247171086258\n",
      "Testing Precision: 0.9812019416162118\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "# Store results\n",
    "training_precision = []\n",
    "testing_precision = []\n",
    "i = 1\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training and testing set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate precision\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    training_precision.append(train_precision)\n",
    "    testing_precision.append(test_precision)\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train precision: {train_precision}')\n",
    "    print(f'Test precision: {test_precision}')\n",
    "    i += 1\n",
    "    \n",
    "# Display results\n",
    "print(\"\\nAVERAGE RESULTS:\")\n",
    "print(\"Training Precision:\", np.mean(training_precision))\n",
    "print(\"Testing Precision:\", np.mean(testing_precision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ced6a-83ab-4528-b1b3-04486a1ea362",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59812cb6-9da5-43e2-824d-15cd3e829441",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "cc732c29-4568-4c46-acf8-a0baec1813e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision: 0.9965034965034965\n",
      "Testing precision: 0.9562043795620438\n",
      "\n",
      "Classification Report Training:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      3573\n",
      "           1       1.00      0.96      0.98       595\n",
      "\n",
      "    accuracy                           0.99      4168\n",
      "   macro avg       0.99      0.98      0.99      4168\n",
      "weighted avg       0.99      0.99      0.99      4168\n",
      "\n",
      "Classification Report Testing:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       894\n",
      "           1       0.96      0.88      0.92       149\n",
      "\n",
      "    accuracy                           0.98      1043\n",
      "   macro avg       0.97      0.94      0.95      1043\n",
      "weighted avg       0.98      0.98      0.98      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=2140)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(f'Training precision: {precision_score(y_train, y_pred_train)}')\n",
    "print(f'Testing precision: {precision_score(y_test, y_pred)}\\n')\n",
    "print(f'Classification Report Training:\\n {classification_report(y_train, y_pred_train)}')\n",
    "print(f'Classification Report Testing:\\n {classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744f549-fa0b-4594-a4f7-a531672b9e5f",
   "metadata": {},
   "source": [
    "### KFold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d2294eb9-2e5a-4d40-b5d0-da25bfd3e6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train precision: 0.9923716460867734\n",
      "Test precision: 0.9776696725201709\n",
      "Fold 2\n",
      "Train precision: 0.9911556370976917\n",
      "Test precision: 0.979647807374383\n",
      "Fold 3\n",
      "Train precision: 0.992846257519895\n",
      "Test precision: 0.9741451389822215\n",
      "Fold 4\n",
      "Train precision: 0.9911555476346708\n",
      "Test precision: 0.9737420964258574\n",
      "Fold 5\n",
      "Train precision: 0.9913637366004152\n",
      "Test precision: 0.977660557072441\n",
      "\n",
      "AVERAGE RESULTS:\n",
      "Training Precision: 0.9917785649878892\n",
      "Testing Precision: 0.9765730544750146\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "# Store results\n",
    "training_precision = []\n",
    "testing_precision = []\n",
    "i = 1\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training and testing set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate precision\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    training_precision.append(train_precision)\n",
    "    testing_precision.append(test_precision)\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train precision: {train_precision}')\n",
    "    print(f'Test precision: {test_precision}')\n",
    "    i += 1\n",
    "    \n",
    "# Display results\n",
    "print(\"\\nAVERAGE RESULTS:\")\n",
    "print(\"Training Precision:\", np.mean(training_precision))\n",
    "print(\"Testing Precision:\", np.mean(testing_precision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dd8b7-dac4-449c-91ff-42fffe80676b",
   "metadata": {},
   "source": [
    "# Test Models on user input \"Emails\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "9abe785c-ebdb-4ccc-9669-0e529390cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input_test(message, rfc, xgb):\n",
    "    input_x = pd.DataFrame({'Message': [message]})\n",
    "\n",
    "    # get engineered features\n",
    "    input_x['freq_urls'] = input_x['Message'].apply(count_url)\n",
    "    input_x['freq_urgent_words'] = input_x['Message'].apply(count_urgency_words, urgency_words=urgency_words)\n",
    "    input_x['freq_exclamation'] = input_x['Message'].apply(count_special_chars, char='!')\n",
    "    input_x['capital_run_length_total'] = input_x['Message'].apply(count_capital_run_length)\n",
    "\n",
    "    # vectorize \n",
    "    X_new = vectorizer.transform(input_x['Message'])\n",
    "    X_new = pd.DataFrame(X_new.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # add engineered features\n",
    "    X_new['freq_urls'] = input_x['freq_urls']\n",
    "    X_new['freq_urgent_words'] = input_x['freq_urgent_words']\n",
    "    X_new['freq_exclamation'] = input_x['freq_exclamation']\n",
    "    X_new['capital_run_length_total'] = input_x['capital_run_length_total']\n",
    "    \n",
    "    # match the data frame to the training (add and remove columns not in the original frame)\n",
    "    missing_cols = set(df_bow.columns) - set(X_new.columns) - {'Category'}\n",
    "    for col in missing_cols:\n",
    "        X_new[col] = 0\n",
    "\n",
    "    X_new = X_new[df_bow.columns.drop('Category')]\n",
    "    predictions = [rfc.predict(X_new), xgb.predict(X_new)]\n",
    "\n",
    "    print('\\n')\n",
    "    if predictions[0] == 0:\n",
    "        print('Random Forest classified as not spam.')\n",
    "    else:\n",
    "        print('Random Forest classified as spam.')\n",
    "    \n",
    "    if predictions[1] == 0:\n",
    "        print('XGBoost classified as not spam.')\n",
    "    else:\n",
    "        print('XGBoost classified as spam.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "656e6dbf-be72-4c71-9495-7fada0437556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random Forest classified as spam.\n",
      "XGBoost classified as spam.\n"
     ]
    }
   ],
   "source": [
    "message = \"Hi, im emailing to inform you that there is a free iphone waiting for you to claim! just go to www.freeiphone.com to claim it! Act now!\"\n",
    "user_input_test(message, rfc, xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "94bba77a-5799-45eb-accf-caa88a3cac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "type email for model classification:\n",
      " Your amazon account was used without your permission! A charge for 5,000 cash was deducted from your bank. Call us now or visit www.amazonnn.com or else you will lose your house!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random Forest classified as spam.\n",
      "XGBoost classified as spam.\n"
     ]
    }
   ],
   "source": [
    "# On your own\n",
    "message = input('type email for model classification:\\n')\n",
    "user_input_test(message, rfc, xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015f2be-d7c8-48c8-a738-656a37762ca5",
   "metadata": {},
   "source": [
    "## Test emails "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd07b9-5b74-442a-a8ec-f6bb24b73804",
   "metadata": {},
   "source": [
    "Please excuse the lack of creativity, and feel free to try your own emails to see my model in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b706a4d-8d91-40d8-9c2d-8c8d2c4376a6",
   "metadata": {},
   "source": [
    "### Example test emails (spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da412a-82f1-4453-8671-22914168ac38",
   "metadata": {},
   "source": [
    "Hi, im emailing to inform you that there is a free iphone waiting for you to claim! just go to www.freeiphone.com to claim it! Act now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732d3cc-3701-4b94-90ed-88b609301ae7",
   "metadata": {},
   "source": [
    "You're invited to the BEST and MOST AMAZING and FREE concert ever! Your favorite artists will attend!! All you need is a credit card! If you register ASAP you will be added to the VIP list and get to sit backstage with your fav artists!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945feb9-8809-42a0-b2db-204aa32d6cc1",
   "metadata": {},
   "source": [
    "Your amazon account was used without your permission! A charge for 5,000 cash was deducted from your bank. Call us now or visit www.amazonnn.com or else you will lose your house!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a38489-cc04-4701-9dfc-9cbf92f9d966",
   "metadata": {},
   "source": [
    "### Example test emails (not spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98d9b2-7a4d-4c96-8698-644db030984c",
   "metadata": {},
   "source": [
    "Hello my name is Brendan from the shipping department. Your order was canceled due to delivery complications. Please contact us so we can fix your order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15d38c-fb7d-4a18-958e-23b876a25650",
   "metadata": {},
   "source": [
    "Hey John, James from accounting said you havent paid for the damage you did to the water heater. Please send that over to me ASAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a4936-df7b-409e-9491-f54a792a03a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
