{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da42a46a-0e2c-498a-86d2-5511089d620f",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9de6792-5fae-4eff-878f-7dd92f7fd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63131368-8ba9-4576-a009-01a26d4f4091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                            Message\n",
       "0            0  Go until jurong point, crazy.. Available only ...\n",
       "1            0                      Ok lar... Joking wif u oni...\n",
       "2            1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3            0  U dun say so early hor... U c already then say...\n",
       "4            0  Nah I don't think he goes to usf, he lives aro...\n",
       "...        ...                                                ...\n",
       "5567         1  This is the 2nd time we have tried 2 contact u...\n",
       "5568         0               Will ü b going to esplanade fr home?\n",
       "5569         0  Pity, * was in mood for that. So...any other s...\n",
       "5570         0  The guy did some bitching but I acted like i'd...\n",
       "5571         0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data\\spam.csv')\n",
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612e91f-22a9-4ab0-87d8-d13566b17105",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970becd-9c2d-4a75-a7f6-128184d24846",
   "metadata": {},
   "source": [
    "### Frequency of URLS and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7506a56c-94db-4d4e-88c3-200e006d561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def count_url(text):\n",
    "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')\n",
    "    return len(url_pattern.findall(text))\n",
    "\n",
    "df['freq_urls'] = df['Message'].apply(count_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a7145-bf07-4b40-aed5-8c3cf2381462",
   "metadata": {},
   "source": [
    "### Frequency of 'Urgent' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55810802-4d02-46ec-b98f-02a43533924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urgency_words = [\n",
    "    \"immediate\", \"urgent\", \"critical\", \"important\", \"now\", \"ASAP\", \"as soon as possible\",\n",
    "    \"emergency\", \"priority\", \"alert\", \"rush\", \"prompt\", \"hasten\", \"swift\", \"instantly\",\n",
    "    \"right away\", \"without delay\", \"high priority\", \"imminent\", \"pressing\", \"time-sensitive\",\n",
    "    \"expedite\", \"top priority\", \"crucial\", \"vital\", \"necessary\", \"quick\", \"speedy\", \"at once\",\n",
    "    \"rapid\", \"flash\", \"instantaneous\", \"accelerated\", \"breakneck\", \"hurry\", \"immediately\",\n",
    "    \"fast-track\", \"at the earliest\", \"act now\", \"don't delay\", \"on the double\", \"without hesitation\",\n",
    "    \"fast\", \"soon\", \"now or never\", \"urgent action\", \"right now\", \"straightaway\", \"double-time\",\n",
    "    \"speed\", \"express\", \"high-priority\", \"pressing need\", \"at your earliest convenience\", \"this instant\",\n",
    "    \"forthwith\", \"like a shot\", \"snap to it\", \"on the spot\", \"no time to lose\", \"no delay\",\n",
    "    \"in a hurry\", \"right this minute\", \"get going\", \"with haste\"\n",
    "]\n",
    "\n",
    "def count_urgency_words(text, urgency_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    count = sum(1 for word in words if word in urgency_words)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d08b5d-26a5-4398-b259-f72ef5e6060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq_urgent_words'] = df['Message'].apply(count_urgency_words, urgency_words=urgency_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ed789-d73a-45b5-843c-47fd3d336f7b",
   "metadata": {},
   "source": [
    "### Capital run length total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d033efb-402e-4f5e-a373-9497b3563a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_run_length(text, min_length_count=2):\n",
    "    capital_runs = re.findall(r'[A-Z]+', text)\n",
    "    run_lengths = [len(run) for run in capital_runs if len(run) >= min_length_count]\n",
    "    return sum(run_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df2e527a-fd4f-425b-8116-b7f193934ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['capital_run_length_total'] = df['Message'].apply(count_capital_run_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4594ecb-7d64-4e82-86d6-455e9825d418",
   "metadata": {},
   "source": [
    "### important special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5869dec-5fc2-40c1-966d-da1da712c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_special_chars(text, char):\n",
    "    return text.count(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f42aa14-25f8-46fc-84fe-b44c590a8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq_exclamation'] = df['Message'].apply(count_special_chars, char='!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdfd71-696e-40de-b5b2-087481b8f921",
   "metadata": {},
   "source": [
    "## Clean text using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "270917bf-7a37-4300-a6c4-e31bd21aa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# stop words\n",
    "# nltk.download('punkt') <------ need these lines to \n",
    "# nltk.download('stopwords') <-- load stopwords\n",
    "stop_words = stopwords.words()\n",
    "stop_words.append('u')\n",
    "stop_words.append('ur')\n",
    "\n",
    "# lemmatizer initialization\n",
    "# nltk.download('averaged_perceptron_tagger') <---- need these lines to downnload\n",
    "# nltk.download('wordnet') <----------------------- wordnet used for lemmitization\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39eb24-7fb3-4edf-9396-a04d556c9239",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3c5b8d-87ee-4b75-a84c-e817bbd36ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_email(text, lemmatizer):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935b61c1-b2dc-4142-8d51-ece32a0f9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message'] = df['Message'].apply(lemmatize_email, lemmatizer=lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849b81a-fd26-4202-bddb-6c2fd8b0d318",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "947c9f77-e822-45a0-a843-fcc68e038434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text, stop_words):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    new_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "838c5277-b6d4-48aa-97b6-c3fc3a903620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message'] = df['Message'].apply(remove_stop_words, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6b2f5-4ff3-4383-9dce-8b7669f010e9",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "768714de-814e-48d0-897f-f441f0242c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# vectorize the emails\n",
    "X = vectorizer.fit_transform(df['Message'])\n",
    "\n",
    "# Create new data frame\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# add engineered features\n",
    "df_bow['freq_urls'] = df['freq_urls']\n",
    "df_bow = df_bow.drop(columns='www') # turns out this column basically has the same counts as url frequency\n",
    "df_bow['freq_urgent_words'] = df['freq_urgent_words']\n",
    "df_bow['freq_exclamation'] = df['freq_exclamation']\n",
    "df_bow['capital_run_length_total'] = df['capital_run_length_total']\n",
    "\n",
    "# add class\n",
    "df_bow['Category'] = df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4929775-d202-4ab8-9175-85b544ea6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find totals in order to slim features\n",
    "df_bow.loc['Total'] = df_bow.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d137eff-bf55-4650-8dec-58960fc272b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features with less than n appearances\n",
    "min_num_appearances = 20 # n\n",
    "cols_to_drop = df_bow.columns[df_bow.loc['Total'] < min_num_appearances] \n",
    "df_bow = df_bow.drop(columns=cols_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c2475ed-35ce-41d7-b749-b7b191cfae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove entries with no appearances of the word freq features while keeping the engineered features intact\n",
    "non_word_freq_columns = {'Category', 'freq_urls', 'freq_urgent_words', 'capital_run_length_total',\n",
    "                         'freq_exclamation'}\n",
    "df_bow = df_bow.loc[~(df_bow[df_bow.drop(columns=non_word_freq_columns).columns] == 0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44af28be-4e46-4ca0-8cd9-ed85562d2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove total row\n",
    "df_bow = df_bow.drop(index='Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee43a1e6-1023-487f-814d-6b69c1d971fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_bow = df_bow.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37438f8-d072-4210-963d-1c7408f48281",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f174e504-1bb4-4c7a-966a-27085a2c96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a5459f9-66f2-4c1d-ace6-acd95cf2e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_bow.drop(columns='Category')\n",
    "y = df_bow['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca849fe-3b45-4899-b8e1-66be5bf65f43",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193799a5-c436-4d82-b7ec-51d400121676",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1e77c63d-3dad-46b2-b520-0c7c49103234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;class_weight&#x27;: [&#x27;balanced&#x27;, &#x27;balanced_subsample&#x27;],\n",
       "                          &#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                          &#x27;max_depth&#x27;: [30, 40, None],\n",
       "                          &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n",
       "                          &#x27;max_leaf_nodes&#x27;: [50, 100, None], &#x27;n_jobs&#x27;: [-1]}],\n",
       "             scoring=&#x27;precision&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;class_weight&#x27;: [&#x27;balanced&#x27;, &#x27;balanced_subsample&#x27;],\n",
       "                          &#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                          &#x27;max_depth&#x27;: [30, 40, None],\n",
       "                          &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n",
       "                          &#x27;max_leaf_nodes&#x27;: [50, 100, None], &#x27;n_jobs&#x27;: [-1]}],\n",
       "             scoring=&#x27;precision&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid=[{'class_weight': ['balanced', 'balanced_subsample'],\n",
       "                          'criterion': ['gini', 'entropy'],\n",
       "                          'max_depth': [30, 40, None],\n",
       "                          'max_features': ['sqrt', 'log2', None],\n",
       "                          'max_leaf_nodes': [50, 100, None], 'n_jobs': [-1]}],\n",
       "             scoring='precision')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [30, 40, None],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'max_leaf_nodes': [50, 100, None],\n",
    "        'class_weight': ['balanced', 'balanced_subsample'],\n",
    "        'n_jobs': [-1]\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, scoring='precision', n_jobs=-1) \n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "48738823-3290-41ff-82ff-f1b4a6b60e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_search.best_params_={'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 40, 'max_features': 'log2', 'max_leaf_nodes': None, 'n_jobs': -1}\n",
      "grid_search.best_score_=0.9906888196607821\n"
     ]
    }
   ],
   "source": [
    "print(f'{grid_search.best_params_=}')\n",
    "print(f'{grid_search.best_score_=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8fe62-4a39-4b0e-9165-01b5e26dca72",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "35116355-2c9a-4144-a34c-4d2e4a957ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision: 1.0\n",
      "Testing precision: 1.0\n",
      "\n",
      "Classification Report Training:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      3207\n",
      "           1       1.00      0.97      0.98       591\n",
      "\n",
      "    accuracy                           0.99      3798\n",
      "   macro avg       1.00      0.98      0.99      3798\n",
      "weighted avg       0.99      0.99      0.99      3798\n",
      "\n",
      "Classification Report Testing:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       802\n",
      "           1       1.00      0.88      0.94       148\n",
      "\n",
      "    accuracy                           0.98       950\n",
      "   macro avg       0.99      0.94      0.96       950\n",
      "weighted avg       0.98      0.98      0.98       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2140)\n",
    "\n",
    "rfc = RandomForestClassifier(oob_score=True, ccp_alpha=0.0001, criterion='entropy', class_weight='balanced', max_depth=40, max_features='log2', n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = rfc.predict(X_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(f'Training precision: {precision_score(y_train, y_pred_train)}')\n",
    "print(f'Testing precision: {precision_score(y_test, y_pred)}\\n')\n",
    "print(f'Classification Report Training:\\n {classification_report(y_train, y_pred_train)}')\n",
    "print(f'Classification Report Testing:\\n {classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc6c9b-3883-4ac2-a84e-cbee8abc3997",
   "metadata": {},
   "source": [
    "### KFold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2b544f08-43f7-4e8d-993f-73001b885696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train precision: 0.9950268307756195\n",
      "Test precision: 0.9845004187334923\n",
      "Fold 2\n",
      "Train precision: 0.9950268307756195\n",
      "Test precision: 0.985515995872033\n",
      "Fold 3\n",
      "Train precision: 0.9950268307756195\n",
      "Test precision: 0.9764644338118023\n",
      "Fold 4\n",
      "Train precision: 0.9952883561769622\n",
      "Test precision: 0.976986253554059\n",
      "Fold 5\n",
      "Train precision: 0.992952184195548\n",
      "Test precision: 0.98125575681284\n",
      "\n",
      "AVERAGE RESULTS:\n",
      "Training Precision: 0.9946642065398738\n",
      "Testing Precision: 0.9809445717568452\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(oob_score=True, ccp_alpha=0.0001, criterion='entropy', class_weight='balanced', max_depth=40, max_features='log2', n_jobs=-1)\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "# Store results\n",
    "training_precision = []\n",
    "testing_precision = []\n",
    "i = 1\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training and testing set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate precision\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    training_precision.append(train_precision)\n",
    "    testing_precision.append(test_precision)\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train precision: {train_precision}')\n",
    "    print(f'Test precision: {test_precision}')\n",
    "    i += 1\n",
    "    \n",
    "# Display results\n",
    "print(\"\\nAVERAGE RESULTS:\")\n",
    "print(\"Training Precision:\", np.mean(training_precision))\n",
    "print(\"Testing Precision:\", np.mean(testing_precision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ced6a-83ab-4528-b1b3-04486a1ea362",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59812cb6-9da5-43e2-824d-15cd3e829441",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc732c29-4568-4c46-acf8-a0baec1813e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision: 0.99822695035461\n",
      "Testing precision: 0.9632352941176471\n",
      "\n",
      "Classification Report Training:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      3207\n",
      "           1       1.00      0.95      0.97       591\n",
      "\n",
      "    accuracy                           0.99      3798\n",
      "   macro avg       0.99      0.98      0.99      3798\n",
      "weighted avg       0.99      0.99      0.99      3798\n",
      "\n",
      "Classification Report Testing:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       802\n",
      "           1       0.96      0.89      0.92       148\n",
      "\n",
      "    accuracy                           0.98       950\n",
      "   macro avg       0.97      0.94      0.95       950\n",
      "weighted avg       0.98      0.98      0.98       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=2140)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(f'Training precision: {precision_score(y_train, y_pred_train)}')\n",
    "print(f'Testing precision: {precision_score(y_test, y_pred)}\\n')\n",
    "print(f'Classification Report Training:\\n {classification_report(y_train, y_pred_train)}')\n",
    "print(f'Classification Report Testing:\\n {classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744f549-fa0b-4594-a4f7-a531672b9e5f",
   "metadata": {},
   "source": [
    "### KFold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2294eb9-2e5a-4d40-b5d0-da25bfd3e6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train precision: 0.9913571904335823\n",
      "Test precision: 0.9776610231057157\n",
      "Fold 2\n",
      "Train precision: 0.9910792824094027\n",
      "Test precision: 0.9809290091796207\n",
      "Fold 3\n",
      "Train precision: 0.992916543617788\n",
      "Test precision: 0.9682389710590344\n",
      "Fold 4\n",
      "Train precision: 0.9921763714708592\n",
      "Test precision: 0.9743915627362295\n",
      "Fold 5\n",
      "Train precision: 0.9921382134316604\n",
      "Test precision: 0.9776616700543416\n",
      "\n",
      "AVERAGE RESULTS:\n",
      "Training Precision: 0.9919335202726586\n",
      "Testing Precision: 0.9757764472269883\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "# Store results\n",
    "training_precision = []\n",
    "testing_precision = []\n",
    "i = 1\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training and testing set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate precision\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    training_precision.append(train_precision)\n",
    "    testing_precision.append(test_precision)\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train precision: {train_precision}')\n",
    "    print(f'Test precision: {test_precision}')\n",
    "    i += 1\n",
    "    \n",
    "# Display results\n",
    "print(\"\\nAVERAGE RESULTS:\")\n",
    "print(\"Training Precision:\", np.mean(training_precision))\n",
    "print(\"Testing Precision:\", np.mean(testing_precision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dd8b7-dac4-449c-91ff-42fffe80676b",
   "metadata": {},
   "source": [
    "# Test Models on user input \"Emails\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9abe785c-ebdb-4ccc-9669-0e529390cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input_test(message, rfc, xgb):\n",
    "    input_x = pd.DataFrame({'Message': [message]})\n",
    "\n",
    "    # get engineered features\n",
    "    input_x['freq_urls'] = input_x['Message'].apply(count_url)\n",
    "    input_x['freq_urgent_words'] = input_x['Message'].apply(count_urgency_words, urgency_words=urgency_words)\n",
    "    input_x['freq_exclamation'] = input_x['Message'].apply(count_special_chars, char='!')\n",
    "    input_x['capital_run_length_total'] = input_x['Message'].apply(count_capital_run_length)\n",
    "\n",
    "    # vectorize \n",
    "    X_new = vectorizer.transform(input_x['Message'])\n",
    "    X_new = pd.DataFrame(X_new.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # add engineered features\n",
    "    X_new['freq_urls'] = input_x['freq_urls']\n",
    "    X_new['freq_urgent_words'] = input_x['freq_urgent_words']\n",
    "    X_new['freq_exclamation'] = input_x['freq_exclamation']\n",
    "    X_new['capital_run_length_total'] = input_x['capital_run_length_total']\n",
    "    \n",
    "    # match the data frame to the training (add and remove columns not in the original frame)\n",
    "    missing_cols = set(df_bow.columns) - set(X_new.columns) - {'Category'}\n",
    "    for col in missing_cols:\n",
    "        X_new[col] = 0\n",
    "\n",
    "    X_new = X_new[df_bow.columns.drop('Category')]\n",
    "    predictions = [rfc.predict(X_new), xgb.predict(X_new)]\n",
    "\n",
    "    print('\\n')\n",
    "    if predictions[0] == 0:\n",
    "        print('Random Forest classified as not spam.')\n",
    "    else:\n",
    "        print('Random Forest classified as spam.')\n",
    "    \n",
    "    if predictions[1] == 0:\n",
    "        print('XGBoost classified as not spam.')\n",
    "    else:\n",
    "        print('XGBoost classified as spam.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "656e6dbf-be72-4c71-9495-7fada0437556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random Forest classified as spam.\n",
      "XGBoost classified as spam.\n"
     ]
    }
   ],
   "source": [
    "message = \"Hi, im emailing to inform you that there is a free iphone waiting for you to claim! just go to www.freeiphone.com to claim it! Act now!\"\n",
    "user_input_test(message, rfc, xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "94bba77a-5799-45eb-accf-caa88a3cac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "type email for model classification:\n",
      " You're invited to the BEST and MOST AMAZING and FREE concert ever! Your favorite artists will attend!! All you need is a credit card! If you register ASAP you will be added to the VIP list and get to sit backstage with your fav artists!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random Forest classified as spam.\n",
      "XGBoost classified as spam.\n"
     ]
    }
   ],
   "source": [
    "# On your own\n",
    "message = input('type email for model classification:\\n')\n",
    "user_input_test(message, rfc, xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015f2be-d7c8-48c8-a738-656a37762ca5",
   "metadata": {},
   "source": [
    "## Test emails "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd07b9-5b74-442a-a8ec-f6bb24b73804",
   "metadata": {},
   "source": [
    "Please excuse the lack of creativity, and feel free to try your own emails to see my model in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b706a4d-8d91-40d8-9c2d-8c8d2c4376a6",
   "metadata": {},
   "source": [
    "### Example test emails (spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da412a-82f1-4453-8671-22914168ac38",
   "metadata": {},
   "source": [
    "Hi, im emailing to inform you that there is a free iphone waiting for you to claim! just go to www.freeiphone.com to claim it! Act now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732d3cc-3701-4b94-90ed-88b609301ae7",
   "metadata": {},
   "source": [
    "You're invited to the BEST and MOST AMAZING and FREE concert ever! Your favorite artists will attend!! All you need is a credit card! If you register ASAP you will be added to the VIP list and get to sit backstage with your fav artists!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945feb9-8809-42a0-b2db-204aa32d6cc1",
   "metadata": {},
   "source": [
    "Your amazon account was used without your permission! A charge for 5,000 cash was deducted from your bank. Call us now or visit www.amazonnn.com or else you will lose your house!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a38489-cc04-4701-9dfc-9cbf92f9d966",
   "metadata": {},
   "source": [
    "### Example test emails (not spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98d9b2-7a4d-4c96-8698-644db030984c",
   "metadata": {},
   "source": [
    "Hello my name is Brendan from the shipping department. Your order was canceled due to delivery complications. Please contact us so we can fix your order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15d38c-fb7d-4a18-958e-23b876a25650",
   "metadata": {},
   "source": [
    "Hey John, James from accounting said you havent paid for the damage you did to the water heater. Please send that over to me ASAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a4936-df7b-409e-9491-f54a792a03a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
